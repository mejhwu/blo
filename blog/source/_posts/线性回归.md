---
title: 线性回归
date: 2018-09-05
tags: 机器学习
mathjax: true
---

# 线性回归

## 基本线性模型

给定有$d$个属性描述的实例$\boldsymbol{x}=(x_1; x_2; ...; x_d)$，其中$x_i$是$\boldsymbol{x}$在第$i$个属性上的取值，线性模型（linear model）试图学得一个通过属性的线性组合来进行预测的函数，即
$$
f(\boldsymbol{x})=w_1x_1 + w_2x_2 + \cdot\cdot\cdot + w_dx_d + b
$$
一般用向量形式写成
$$
f(\boldsymbol{x})=\boldsymbol{w}^T \boldsymbol{x} + b
$$
其中$\boldsymbol{w}=(w_1; w_2; ...; w_d)$. $\boldsymbol{w}$和$b$学得之后，模型就可以确定。

## 线性回归

给定数据集$D=\{(\boldsymbol{x_1}, y_1), (\boldsymbol{x_2}, y_2), \cdots, (\boldsymbol{x_m}, y_m),\}$，其中$\boldsymbol{x_i}=(x_{i1}, x_{i2}, \cdots, x_{id}), y_i \in \mathbb{R}$. “线性回归”试图学的一个线性模型尽可能准确的预测实值输出标记。

对于离散值，若属性之间存在“序”关系，可通过连续化将其转化为连续值，如三属性值“高度”的取值为“高” “中” “矮”,可转化为{1.0, .0.5, 0.0}；若属性值之间不存在序关系，假定有$k$个属性值，则通常转化为$k$为向量。

线性回归试图学得$f(x_i) = \boldsymbol{w^T} \boldsymbol{x}_i + b$，使得$f(\boldsymbol{x}_i) \simeq y_i$。

 回归问题中常用的损失函数（性能度量）是均方误差，令$\boldsymbol{\theta}=(\boldsymbol{w}, b)$, 则均方误差可表示为：
$$
\mathrm{J(\boldsymbol{\theta})}=\frac{1}{2}\sum_{i=1}^{m}(f(\boldsymbol{x}_i) - y_i)^2
$$

### 梯度下降

最小化$\mathrm{J}(\boldsymbol{\theta})$是一个最优化问题，可以采用梯度下降算法或牛顿法求解。

### 最小二乘法

首先考虑$x$只有一个属性，可以试图让均方误差最小化，即
$$
\begin{align}
(w^*, b^*) & =  \arg\min_{(w, b)}\sum_{i=1}^{m}(f(x_i) - y_i)^2  \\
                   & = \arg\min_{(w,b)}\sum_{i=1}^m(y_I - wx_i -b)^2
\end{align}
$$
基于均方误差最小化来进行模型求解的的方法称为“最小二乘法”（least square method). 在线性回归中，最小二乘法就是找到一条直线，使得所有样本到直线上的欧式距离之和最小。

求解$w$和$b$使$E_{(w,b)}=\sum_{i=1}^m(y_i-wx_i-b)^2$最小化的过程，称为线性回归模型的最小二乘“参数估计“(parameter estimation). 将$E_{(w,b)}$分别对$w$和$b$求导，得到
$$
\frac{\partial E_{(w,b)}}{\partial w} = 2(w\sum_{i=1}^mx_i^2 - \sum_{i=1}^m (y_i - b)x_i)
$$

$$
\frac{\partial E_{(w,b)}}{\partial b} = 2(mb - \sum_{i=1}^m(y_i-wx_i))
$$

令是（6）和式（7）分别等于0，可得到$w$和$b$的最优解的闭式解
$$
w=\frac{\sum_{i=1}^m y_i(x_i - \bar{x})}{\sum_{i=1}^mx_i^2 - \frac{1}{m}(\sum_{i=1}^mx_i)^2}
$$

$$
b=\frac{1}{m}\sum_{i=1}^m(y_i - wx_i)
$$

当$\boldsymbol{x}$由$d$个属性描述时，把数据集$D$表示为一个$m \times (d + 1)$大小的矩阵$\mathrm{X}$，每一行的前$d$个属性对应于样本的$d$个属性值，最后一个元素恒置为1，即
$$
\mathrm{X} =   \left[ \begin{matrix} 
			x_{11} & x_{12} & \cdots & x_{1d} & 1 \\
            	      x_{21} & x_{22} & \cdots & x_{2d} & 1 \\
            	      \vdots & \vdots & \ddots & \vdots & \vdots \\
            	      x_{m1} & x_{m2} & \cdots & x_{md} & 1
			\end{matrix} \right]
		=   \left[ \begin{matrix} 
			\boldsymbol{x}_1^T & 1 \\
            	      \boldsymbol{x}_2^T  & 1 \\
            	      \vdots & \vdots \\
            	      \boldsymbol{x}_m^T  & 1
			\end{matrix} \right]
$$
将标记写为向量形式$\boldsymbol{y}=(y_1, y_2, \cdots, y_m)$， 则有
$$
\boldsymbol{\theta}^* = \arg\min_{\boldsymbol{\theta}}(\boldsymbol{y}-\mathrm{X}\boldsymbol{\theta})^T(\boldsymbol{y}-\mathrm{X}\boldsymbol{\theta})
$$
令$E_{\boldsymbol{\theta}}=(\boldsymbol{y}-\mathrm{X}\boldsymbol{\theta})^T(\boldsymbol{y}-\mathrm{X}\boldsymbol{\theta})$，对$\boldsymbol{\theta}$求导得到
$$
\frac{\partial{E_{\boldsymbol{\theta}}}}{\partial{\boldsymbol{\theta}}}=2 \mathrm{X}^T(\mathrm{X}\boldsymbol{\theta}-\boldsymbol{y})
$$
令上式等于零，即可求得$\boldsymbol{\theta}$的最优解的闭式解。



